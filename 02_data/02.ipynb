{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d95f841a-63c9-41d4-aea1-496b3d2024dd",
   "metadata": {},
   "source": [
    "**LLM Workshop 2024 by Sebastian Raschka**\n",
    "\n",
    "This code is based on *Build a Large Language Model (From Scratch)*, [https://github.com/rasbt/LLMs-from-scratch](https://github.com/rasbt/LLMs-from-scratch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25aa40e3-5109-433f-9153-f5770531fe94",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "# 2) Understanding LLM Input Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d5d2c0-cba8-404e-9bf3-71a218cae3cf",
   "metadata": {},
   "source": [
    "Packages that are being used in this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d1305cf-12d5-46fe-a2c9-36fb71c5b3d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.4.1\n",
      "tiktoken version: 0.8.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "\n",
    "print(\"torch version:\", version(\"torch\"))\n",
    "print(\"tiktoken version:\", version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a42fbfd-e3c2-43c2-bc12-f5f870a0b10a",
   "metadata": {},
   "source": [
    "- This notebook provides a brief overview of the data preparation and sampling procedures to get input data \"ready\" for an LLM\n",
    "- Understanding what the input data looks like is a great first step towards understanding how LLMs work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628b2922-594d-4ff9-bd82-04f1ebdf41f5",
   "metadata": {},
   "source": [
    "<img src=\"./figures/01.png\" width=\"1000px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddbb984-8d23-40c5-bbfa-c3c379e7eec3",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "# 2.1 Tokenizing text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c90731-7dc9-4cd3-8c4a-488e33b48e80",
   "metadata": {},
   "source": [
    "- In this section, we tokenize text, which means breaking text into smaller units, such as individual words and punctuation characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09872fdb-9d4e-40c4-949d-52a01a43ec4b",
   "metadata": {},
   "source": [
    "<img src=\"figures/02.png\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cceaa18-833d-46b6-b211-b20c53902805",
   "metadata": {},
   "source": [
    "- Load raw text we want to work with\n",
    "- [The Verdict by Edith Wharton](https://en.wikisource.org/wiki/The_Verdict) is a public domain short story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8a769e87-470a-48b9-8bdb-12841b416198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of character: 59342\n",
      "If you collected lists of techniques for doing great work in a lot of different fields, what would \n"
     ]
    }
   ],
   "source": [
    "# with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "#     raw_text = f.read()\n",
    "\n",
    "with open(\"how-to-do-great-work.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "    \n",
    "print(\"Total number of character:\", len(raw_text))\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b971a46-ac03-4368-88ae-3f20279e8f4e",
   "metadata": {},
   "source": [
    "- The goal is to tokenize and embed this text for an LLM\n",
    "- Let's develop a simple tokenizer based on some simple sample text that we can then later apply to the text above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbe9330-b587-4262-be9f-497a84ec0e8a",
   "metadata": {},
   "source": [
    "<img src=\"figures/03.png\" width=\"690px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3daa1687-2c08-485a-87cc-a93c2f9586d7",
   "metadata": {},
   "source": [
    "- The following regular expression will split on whitespaces and punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "737dd5b0-9dbb-4a97-9ae4-3482c8c04be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['If', ' ', 'you', ' ', 'collected', ' ', 'lists', ' ', 'of', ' ', 'techniques', ' ', 'for', ' ', 'doing', ' ', 'great', ' ', 'work', ' ', 'in', ' ', 'a', ' ', 'lot', ' ', 'of', ' ', 'different', ' ', 'fields', ',', ' ', 'what', ' ', 'would', ' ', 'the']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item for item in preprocessed if item]\n",
    "print(preprocessed[:38])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "35db7b5e-510b-4c45-995f-f5ad64a8e19c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 23383\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of tokens:\", len(preprocessed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2cc59b7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n',\n",
       " ' ',\n",
       " '!',\n",
       " '\"',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " ',',\n",
       " '.',\n",
       " '14',\n",
       " '15',\n",
       " '18',\n",
       " '21',\n",
       " '50',\n",
       " '7',\n",
       " '99%',\n",
       " ':',\n",
       " ';',\n",
       " '?',\n",
       " 'A',\n",
       " 'Affectation',\n",
       " 'Am',\n",
       " 'Ambition',\n",
       " 'Ambitious',\n",
       " 'An',\n",
       " 'And',\n",
       " 'Another',\n",
       " 'As',\n",
       " 'Aspies',\n",
       " 'At',\n",
       " 'Avoid',\n",
       " 'Be',\n",
       " 'Begin',\n",
       " 'Being',\n",
       " 'Believe',\n",
       " 'Big',\n",
       " 'Boldly',\n",
       " 'Broken',\n",
       " 'But',\n",
       " 'By',\n",
       " 'Can',\n",
       " 'Changing',\n",
       " 'Colleagues',\n",
       " 'Competition',\n",
       " 'Consciously',\n",
       " 'Copernicus',\n",
       " 'Corollary',\n",
       " 'Curiosity',\n",
       " 'Darwin',\n",
       " 'Develop',\n",
       " 'Do',\n",
       " 'Doing',\n",
       " 'Don',\n",
       " 'Durer',\n",
       " 'Einstein',\n",
       " 'Either',\n",
       " 'Elegance',\n",
       " 'English',\n",
       " 'Even',\n",
       " 'Every',\n",
       " 'Everyone',\n",
       " 'Exception',\n",
       " 'Fame',\n",
       " 'Few',\n",
       " 'Fields',\n",
       " 'Finishing',\n",
       " 'Five',\n",
       " 'Following',\n",
       " 'For',\n",
       " 'Fortunately',\n",
       " 'Four',\n",
       " 'From',\n",
       " 'God',\n",
       " 'Good',\n",
       " 'Great',\n",
       " 'Growing',\n",
       " 'Have',\n",
       " 'Having',\n",
       " 'He',\n",
       " 'Here',\n",
       " 'History',\n",
       " 'How',\n",
       " 'Husband',\n",
       " 'I',\n",
       " 'Ideally',\n",
       " 'If',\n",
       " 'In',\n",
       " 'Indeed',\n",
       " 'Inexperience',\n",
       " 'Informality',\n",
       " 'Instead',\n",
       " 'Interest',\n",
       " 'Is',\n",
       " 'It',\n",
       " 'Just',\n",
       " 'Knowledge',\n",
       " 'Laborious',\n",
       " 'Learning',\n",
       " 'Lego',\n",
       " 'Let',\n",
       " 'Lots',\n",
       " 'Luck',\n",
       " 'Many',\n",
       " 'Mathematical',\n",
       " 'Maxwell',\n",
       " 'Morale',\n",
       " 'Most',\n",
       " 'Much',\n",
       " 'Negative',\n",
       " 'Nerds',\n",
       " 'Never',\n",
       " 'New',\n",
       " 'Newton',\n",
       " 'Nor',\n",
       " 'Not',\n",
       " 'Notice',\n",
       " 'Now',\n",
       " 'Obviously',\n",
       " 'Oddly',\n",
       " 'Often',\n",
       " 'Old',\n",
       " 'Once',\n",
       " 'One',\n",
       " 'Opportunists',\n",
       " 'Or',\n",
       " 'Original',\n",
       " 'Originality',\n",
       " 'Out',\n",
       " 'Paradoxical',\n",
       " 'Part',\n",
       " 'Partly',\n",
       " 'People',\n",
       " 'Per-project',\n",
       " 'Planning',\n",
       " 'Presumably',\n",
       " 'Pretentiousness',\n",
       " 'Probably',\n",
       " 'Projects',\n",
       " 'Quality',\n",
       " 'Questions',\n",
       " 'Religions',\n",
       " 'Renaissance',\n",
       " 'Rules',\n",
       " 'Running',\n",
       " 'Schools',\n",
       " 'Seeing',\n",
       " 'Seek',\n",
       " 'Several',\n",
       " 'Shakespeare',\n",
       " 'Similar',\n",
       " 'Similarly',\n",
       " 'Since',\n",
       " 'So',\n",
       " 'Solving',\n",
       " 'Some',\n",
       " 'Something',\n",
       " 'Sometimes',\n",
       " 'Sorry',\n",
       " 'Spend',\n",
       " 'Steps',\n",
       " 'Strange',\n",
       " 'Strangely',\n",
       " 'Strictness',\n",
       " 'Style',\n",
       " 'Surprisingly',\n",
       " 'Take',\n",
       " 'Talking',\n",
       " 'Testament',\n",
       " 'That',\n",
       " 'The',\n",
       " 'Then',\n",
       " 'There',\n",
       " 'They',\n",
       " 'Think',\n",
       " 'This',\n",
       " 'Though',\n",
       " 'Till',\n",
       " 'To',\n",
       " 'True',\n",
       " 'Try',\n",
       " 'Trying',\n",
       " 'Ultimately',\n",
       " 'Unanswered',\n",
       " 'Understand',\n",
       " 'Unfashionable',\n",
       " 'Until',\n",
       " 'Use',\n",
       " 'Usually',\n",
       " 'Watt',\n",
       " 'We',\n",
       " 'What',\n",
       " 'Whatever',\n",
       " 'When',\n",
       " 'Whereas',\n",
       " 'Which',\n",
       " 'Why',\n",
       " 'With',\n",
       " 'Work',\n",
       " 'Working',\n",
       " 'Write',\n",
       " 'Writing',\n",
       " 'Yes',\n",
       " 'You',\n",
       " 'Your',\n",
       " '[10]',\n",
       " '[11]',\n",
       " '[12]',\n",
       " '[13]',\n",
       " '[14]',\n",
       " '[15]',\n",
       " '[16]',\n",
       " '[17]',\n",
       " '[18]',\n",
       " '[19]',\n",
       " '[1]',\n",
       " '[20]',\n",
       " '[21]',\n",
       " '[22]',\n",
       " '[23]',\n",
       " '[24]',\n",
       " '[25]',\n",
       " '[26]',\n",
       " '[27]',\n",
       " '[28]',\n",
       " '[29]',\n",
       " '[2]',\n",
       " '[3]',\n",
       " '[4]',\n",
       " '[5]',\n",
       " '[6]',\n",
       " '[7]',\n",
       " '[8]',\n",
       " '[9]',\n",
       " 'a',\n",
       " 'abandon',\n",
       " 'ability',\n",
       " 'able',\n",
       " 'about',\n",
       " 'absence',\n",
       " 'accepted',\n",
       " 'accommodate',\n",
       " 'according',\n",
       " 'achieve',\n",
       " 'achievements',\n",
       " 'acquire',\n",
       " 'acquired',\n",
       " 'across',\n",
       " 'action',\n",
       " 'activation',\n",
       " 'acts',\n",
       " 'actual',\n",
       " 'actually',\n",
       " 'additional',\n",
       " 'adds',\n",
       " 'adjacent',\n",
       " 'admire',\n",
       " 'admissions',\n",
       " 'admit',\n",
       " 'admitted',\n",
       " 'adopt',\n",
       " 'adulthood',\n",
       " 'advance',\n",
       " 'advantage',\n",
       " 'advantages',\n",
       " 'advice',\n",
       " 'affect',\n",
       " 'affectation',\n",
       " 'affected',\n",
       " 'afford',\n",
       " 'afraid',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'age',\n",
       " 'aggressively',\n",
       " 'ahead',\n",
       " 'aim',\n",
       " 'aiming',\n",
       " 'ain',\n",
       " 'alarms',\n",
       " 'alive',\n",
       " 'all',\n",
       " 'allow',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'already',\n",
       " 'also',\n",
       " 'alternative',\n",
       " 'always',\n",
       " 'ambition',\n",
       " 'ambitious',\n",
       " 'among',\n",
       " 'amount',\n",
       " 'an',\n",
       " 'analogies',\n",
       " 'and',\n",
       " 'angle',\n",
       " 'another',\n",
       " 'answer',\n",
       " 'answered',\n",
       " 'answering',\n",
       " 'answers',\n",
       " 'any',\n",
       " 'anyone',\n",
       " 'anything',\n",
       " 'anyway',\n",
       " 'apparent',\n",
       " 'appears',\n",
       " 'applied',\n",
       " 'approach',\n",
       " 'aptitude',\n",
       " 'are',\n",
       " 'area',\n",
       " 'areas',\n",
       " 'aren',\n",
       " 'arise',\n",
       " 'around',\n",
       " 'arrange',\n",
       " 'arrive',\n",
       " 'artifacts',\n",
       " 'artistic',\n",
       " 'artists',\n",
       " 'arts',\n",
       " 'as',\n",
       " 'aside',\n",
       " 'ask',\n",
       " 'asked',\n",
       " 'asking',\n",
       " 'associated',\n",
       " 'assume',\n",
       " 'assumes',\n",
       " 'assuming',\n",
       " 'assumption',\n",
       " 'assumptions',\n",
       " 'astray',\n",
       " 'astronomers',\n",
       " 'at',\n",
       " 'attached',\n",
       " 'attack',\n",
       " 'attempt',\n",
       " 'attention',\n",
       " 'audacity',\n",
       " 'audience',\n",
       " 'authority',\n",
       " 'average',\n",
       " 'avoid',\n",
       " 'avoiding',\n",
       " 'away',\n",
       " 'awkwardly',\n",
       " 'b',\n",
       " 'back',\n",
       " 'backtrack',\n",
       " 'backtracking',\n",
       " 'bad',\n",
       " 'badly',\n",
       " 'balloon',\n",
       " 'based',\n",
       " 'bash',\n",
       " 'basically',\n",
       " 'basis',\n",
       " 'be',\n",
       " 'beat',\n",
       " 'because',\n",
       " 'become',\n",
       " 'becomes',\n",
       " 'bed',\n",
       " 'been',\n",
       " 'before',\n",
       " 'began',\n",
       " 'begin',\n",
       " 'beginning',\n",
       " 'behavior',\n",
       " 'being',\n",
       " 'beliefs',\n",
       " 'benefit',\n",
       " 'best',\n",
       " 'bet',\n",
       " 'bets',\n",
       " 'better',\n",
       " 'between',\n",
       " 'beyond',\n",
       " 'bias',\n",
       " 'big',\n",
       " 'bigger',\n",
       " 'biggest',\n",
       " 'biographies',\n",
       " 'bit',\n",
       " 'bitter',\n",
       " 'blocks',\n",
       " 'blown',\n",
       " 'body',\n",
       " 'boils',\n",
       " 'boldness',\n",
       " 'book',\n",
       " 'books',\n",
       " 'bore',\n",
       " 'bosses',\n",
       " 'both',\n",
       " 'bounce',\n",
       " 'brain',\n",
       " 'break',\n",
       " 'breakage',\n",
       " 'breaking',\n",
       " 'breaks',\n",
       " 'breath',\n",
       " 'briefly',\n",
       " 'bring',\n",
       " 'broken',\n",
       " 'brought',\n",
       " 'bud',\n",
       " 'build',\n",
       " 'building',\n",
       " 'built',\n",
       " 'burden',\n",
       " 'burdensome',\n",
       " 'bursting',\n",
       " 'business',\n",
       " 'busy',\n",
       " 'but',\n",
       " 'by',\n",
       " 'calculation',\n",
       " 'calculus',\n",
       " 'call',\n",
       " 'called',\n",
       " 'camouflages',\n",
       " 'can',\n",
       " 'care',\n",
       " 'carry',\n",
       " 'carrying',\n",
       " 'case',\n",
       " 'cases',\n",
       " 'cause',\n",
       " 'century',\n",
       " 'certain',\n",
       " 'certainty',\n",
       " 'chance',\n",
       " 'change',\n",
       " 'character',\n",
       " 'chase',\n",
       " 'chasing',\n",
       " 'check',\n",
       " 'cherished',\n",
       " 'child',\n",
       " 'childhood',\n",
       " 'children',\n",
       " 'choice',\n",
       " 'choose',\n",
       " 'choosing',\n",
       " 'class',\n",
       " 'classes',\n",
       " 'clear',\n",
       " 'cleverer',\n",
       " 'close',\n",
       " 'closely',\n",
       " 'closer',\n",
       " 'clues',\n",
       " 'clusters',\n",
       " 'collaborating',\n",
       " 'colleagues',\n",
       " 'collected',\n",
       " 'collections',\n",
       " 'colorful',\n",
       " 'combination',\n",
       " 'combine',\n",
       " 'come',\n",
       " 'comes',\n",
       " 'comfortable',\n",
       " 'comically',\n",
       " 'command',\n",
       " 'commit',\n",
       " 'committees',\n",
       " 'common',\n",
       " 'comparatively',\n",
       " 'competition',\n",
       " 'competitors',\n",
       " 'completely',\n",
       " 'complicated',\n",
       " 'component',\n",
       " 'composition',\n",
       " 'compounds',\n",
       " 'conceived',\n",
       " 'concentrated',\n",
       " 'conceptual',\n",
       " 'conceptually',\n",
       " 'concrete',\n",
       " 'condition',\n",
       " 'conditions',\n",
       " 'conduit',\n",
       " 'confidence',\n",
       " 'confusion',\n",
       " 'connections',\n",
       " 'conscious',\n",
       " 'consciously',\n",
       " 'conservatism',\n",
       " 'conservative',\n",
       " 'considering',\n",
       " 'consistency',\n",
       " 'consistent',\n",
       " 'consistently',\n",
       " 'consists',\n",
       " 'conspicuous',\n",
       " 'constrain',\n",
       " 'contemporaries',\n",
       " 'context',\n",
       " 'contiguous',\n",
       " 'contradict',\n",
       " 'contradiction',\n",
       " 'controversial',\n",
       " 'conventional',\n",
       " 'converge',\n",
       " 'cool',\n",
       " 'coordinate',\n",
       " 'copy',\n",
       " 'copying',\n",
       " 'core',\n",
       " 'correctly',\n",
       " 'corresponding',\n",
       " 'cost',\n",
       " 'could',\n",
       " 'couldn',\n",
       " 'countries',\n",
       " 'counts',\n",
       " 'course',\n",
       " 'crack',\n",
       " 'cram',\n",
       " 'crazy',\n",
       " 'create',\n",
       " 'creates',\n",
       " 'creating',\n",
       " 'creation',\n",
       " 'creative',\n",
       " 'creativity',\n",
       " 'criterion',\n",
       " 'critical',\n",
       " 'criticisms',\n",
       " 'criticize',\n",
       " 'critics',\n",
       " 'crossed',\n",
       " 'cultivate',\n",
       " 'cultivating',\n",
       " 'culture',\n",
       " 'cumulative',\n",
       " 'curiosity',\n",
       " 'curious',\n",
       " 'current',\n",
       " 'currents',\n",
       " 'curve',\n",
       " 'cut',\n",
       " 'cycle',\n",
       " 'cynical',\n",
       " 'd',\n",
       " 'damage',\n",
       " 'dance',\n",
       " 'dangerous',\n",
       " 'dangers',\n",
       " 'dating',\n",
       " 'day',\n",
       " 'daydreaming',\n",
       " 'days',\n",
       " 'dead',\n",
       " 'deal',\n",
       " 'decide',\n",
       " 'decided',\n",
       " 'deciding',\n",
       " 'decision',\n",
       " 'decisions',\n",
       " 'decrease',\n",
       " 'dedicated',\n",
       " 'deep',\n",
       " 'deeply',\n",
       " 'definite',\n",
       " 'definitely',\n",
       " 'definition',\n",
       " 'degree',\n",
       " 'deliberate',\n",
       " 'deliberately',\n",
       " 'delight',\n",
       " 'demoralize',\n",
       " 'denial',\n",
       " 'departments',\n",
       " 'dependent',\n",
       " 'depends',\n",
       " 'depressing',\n",
       " 'depth-first',\n",
       " 'derivative',\n",
       " 'describe',\n",
       " 'described',\n",
       " 'designed',\n",
       " 'designs',\n",
       " 'desire',\n",
       " 'despite',\n",
       " 'destroy',\n",
       " 'deter',\n",
       " 'diagonally',\n",
       " 'did',\n",
       " 'didn',\n",
       " 'difference',\n",
       " 'different',\n",
       " 'differs',\n",
       " 'difficult',\n",
       " 'difficulties',\n",
       " 'diligence',\n",
       " 'diligently',\n",
       " 'diminishing',\n",
       " 'direct',\n",
       " 'direction',\n",
       " 'directly',\n",
       " 'discard',\n",
       " 'discover',\n",
       " 'discovered',\n",
       " 'discoverers',\n",
       " 'discoveries',\n",
       " 'discovering',\n",
       " 'discovery',\n",
       " 'dishonest',\n",
       " 'dishonesty',\n",
       " 'dislodges',\n",
       " 'dismissed',\n",
       " 'distance',\n",
       " 'distant',\n",
       " 'distinctive',\n",
       " 'distinguish',\n",
       " 'distinguishes',\n",
       " 'distortions',\n",
       " 'distraction',\n",
       " 'distractions',\n",
       " 'distribute',\n",
       " 'divide',\n",
       " 'divorced',\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " 'doing',\n",
       " 'don',\n",
       " 'done',\n",
       " 'doubt',\n",
       " 'down',\n",
       " 'drawn',\n",
       " 'draws',\n",
       " 'dream',\n",
       " 'dreams',\n",
       " 'drift',\n",
       " 'drive',\n",
       " 'driving',\n",
       " 'drugs',\n",
       " 'each',\n",
       " 'early',\n",
       " 'earnest',\n",
       " 'earnestness',\n",
       " 'earth',\n",
       " 'easier',\n",
       " 'easiest',\n",
       " 'easy',\n",
       " 'eating',\n",
       " 'economically',\n",
       " 'economy',\n",
       " 'edges',\n",
       " 'education',\n",
       " 'educational',\n",
       " 'effect',\n",
       " 'effective',\n",
       " 'efficiency',\n",
       " 'efficient',\n",
       " 'effort',\n",
       " 'efforts',\n",
       " 'either',\n",
       " 'eject',\n",
       " 'elegance',\n",
       " 'elegant',\n",
       " 'eliminating',\n",
       " 'else',\n",
       " 'elsewhere',\n",
       " 'embrace',\n",
       " 'embraces',\n",
       " 'eminent',\n",
       " 'empirical',\n",
       " 'empirically',\n",
       " 'encounter',\n",
       " 'encountered',\n",
       " 'encourage',\n",
       " 'end',\n",
       " 'energy',\n",
       " 'engaged',\n",
       " 'engaging',\n",
       " 'engine',\n",
       " 'enjoy',\n",
       " 'enough',\n",
       " 'entail',\n",
       " 'entails',\n",
       " 'equally',\n",
       " 'equations',\n",
       " 'err',\n",
       " 'error',\n",
       " 'escape',\n",
       " 'escapism',\n",
       " 'especially',\n",
       " 'essay',\n",
       " 'essence',\n",
       " 'essential',\n",
       " 'established',\n",
       " 'even',\n",
       " 'evenly',\n",
       " 'eventually',\n",
       " 'ever',\n",
       " 'every',\n",
       " 'everyday',\n",
       " 'everyone',\n",
       " 'everything',\n",
       " 'evidence',\n",
       " 'evil',\n",
       " 'evolve',\n",
       " 'evolves',\n",
       " 'evolving',\n",
       " 'exacerbates',\n",
       " 'exactly',\n",
       " 'exaggerate',\n",
       " 'example',\n",
       " 'examples',\n",
       " 'except',\n",
       " 'exception',\n",
       " 'exceptionally',\n",
       " 'excessively',\n",
       " 'excited',\n",
       " 'exciting',\n",
       " 'excitingly',\n",
       " 'excitingness',\n",
       " 'executing',\n",
       " 'exercise',\n",
       " 'exercising',\n",
       " 'exist',\n",
       " 'existing',\n",
       " 'exists',\n",
       " 'expands',\n",
       " 'expect',\n",
       " 'expected',\n",
       " 'expend',\n",
       " 'experience',\n",
       " 'experiment',\n",
       " 'experiments',\n",
       " 'expertise',\n",
       " 'experts',\n",
       " 'explicit',\n",
       " 'explicitly',\n",
       " 'explore',\n",
       " 'explored',\n",
       " 'exploring',\n",
       " 'explosion',\n",
       " 'exponential',\n",
       " 'exponentially',\n",
       " 'exposing',\n",
       " 'express',\n",
       " 'extent',\n",
       " 'extraordinary',\n",
       " 'extreme',\n",
       " 'eye',\n",
       " 'eyes',\n",
       " 'face',\n",
       " 'fact',\n",
       " 'factors',\n",
       " 'fail',\n",
       " 'failing',\n",
       " 'fails',\n",
       " 'failure',\n",
       " 'fake',\n",
       " 'fakeness',\n",
       " 'fall',\n",
       " 'falling',\n",
       " 'false',\n",
       " 'famous',\n",
       " 'famously',\n",
       " 'fan',\n",
       " 'fans',\n",
       " 'far',\n",
       " 'fashion',\n",
       " 'fashionable',\n",
       " 'fast',\n",
       " 'faster',\n",
       " 'fatigue',\n",
       " 'fear',\n",
       " 'feature',\n",
       " 'features',\n",
       " 'feedback',\n",
       " 'feeds',\n",
       " 'feel',\n",
       " 'feels',\n",
       " 'felt',\n",
       " 'few',\n",
       " 'fiction',\n",
       " 'field',\n",
       " 'fields',\n",
       " 'figure',\n",
       " 'figuring',\n",
       " 'filter',\n",
       " 'filters',\n",
       " 'final',\n",
       " 'find',\n",
       " 'finding',\n",
       " 'fine',\n",
       " 'finish',\n",
       " 'first',\n",
       " 'fit',\n",
       " 'five',\n",
       " 'fix',\n",
       " 'fixing',\n",
       " 'flat',\n",
       " 'flaw',\n",
       " 'flaws',\n",
       " 'flexible',\n",
       " 'focus',\n",
       " 'focused',\n",
       " 'focusing',\n",
       " 'follow',\n",
       " 'following',\n",
       " 'fool',\n",
       " 'for',\n",
       " 'force',\n",
       " 'forces',\n",
       " 'forget',\n",
       " 'form',\n",
       " 'formality',\n",
       " 'former',\n",
       " 'forms',\n",
       " 'forward',\n",
       " 'found',\n",
       " 'four',\n",
       " 'fractal',\n",
       " 'fractally',\n",
       " 'frauds',\n",
       " 'freakishly',\n",
       " 'free',\n",
       " 'freedom',\n",
       " 'fresh',\n",
       " 'friends',\n",
       " 'frightening',\n",
       " 'frivolous',\n",
       " 'from',\n",
       " 'front',\n",
       " 'frontal',\n",
       " 'frontier',\n",
       " 'frontiers',\n",
       " 'fruitful',\n",
       " 'full',\n",
       " 'fully',\n",
       " 'fun',\n",
       " 'further',\n",
       " 'furtively',\n",
       " 'future',\n",
       " 'game',\n",
       " 'gaps',\n",
       " 'gatekeeper',\n",
       " 'general',\n",
       " 'generally',\n",
       " 'generate',\n",
       " 'genuinely',\n",
       " 'get',\n",
       " 'gets',\n",
       " 'give',\n",
       " 'gives',\n",
       " 'giving',\n",
       " 'go',\n",
       " 'goal',\n",
       " 'goes',\n",
       " 'going',\n",
       " 'gold',\n",
       " 'good',\n",
       " 'got',\n",
       " 'gotten',\n",
       " 'grammatically',\n",
       " 'granted',\n",
       " 'grasp',\n",
       " 'gratuitously',\n",
       " 'great',\n",
       " 'greater',\n",
       " 'greatest',\n",
       " 'grew',\n",
       " 'grinder',\n",
       " 'grounds',\n",
       " 'group',\n",
       " 'grow',\n",
       " 'grows',\n",
       " 'growth',\n",
       " 'guess',\n",
       " 'guide',\n",
       " 'habit',\n",
       " 'habits',\n",
       " 'hacking',\n",
       " 'had',\n",
       " 'half',\n",
       " 'hand',\n",
       " 'handful',\n",
       " 'happen',\n",
       " 'happening',\n",
       " 'happens',\n",
       " 'happier',\n",
       " 'happy',\n",
       " 'hard',\n",
       " 'harder',\n",
       " 'hardest',\n",
       " 'hardly',\n",
       " 'has',\n",
       " 'hasn',\n",
       " 'hate',\n",
       " 'have',\n",
       " 'having',\n",
       " 'he',\n",
       " 'head',\n",
       " 'headwinds',\n",
       " 'health',\n",
       " 'heard',\n",
       " 'heart',\n",
       " 'heavy',\n",
       " 'heliocentric',\n",
       " 'help',\n",
       " 'helps',\n",
       " 'here',\n",
       " 'hidden',\n",
       " 'high',\n",
       " 'higher',\n",
       " 'history',\n",
       " 'hobbies',\n",
       " 'hobbyist',\n",
       " 'hobbyists',\n",
       " 'hold',\n",
       " 'holds',\n",
       " 'honest',\n",
       " 'hours',\n",
       " 'how',\n",
       " 'however',\n",
       " 'huge',\n",
       " 'human',\n",
       " 'hundred',\n",
       " 'hundreds',\n",
       " 'hurry',\n",
       " 'hype',\n",
       " 'idea',\n",
       " 'ideas',\n",
       " 'identical',\n",
       " 'identity',\n",
       " 'idiom',\n",
       " 'idly',\n",
       " 'if',\n",
       " 'ignorance',\n",
       " 'ignore',\n",
       " 'ignores',\n",
       " 'imaginary',\n",
       " 'imagination',\n",
       " 'imitate',\n",
       " 'immensely',\n",
       " 'immunity',\n",
       " 'implications',\n",
       " 'implicit',\n",
       " 'implicitly',\n",
       " 'implies',\n",
       " 'importance',\n",
       " 'important',\n",
       " 'impress',\n",
       " 'impression',\n",
       " 'impressive',\n",
       " 'impressiveness',\n",
       " 'in',\n",
       " 'including',\n",
       " 'incomplete',\n",
       " 'increase',\n",
       " 'increases',\n",
       " 'increasingly',\n",
       " 'independent-minded',\n",
       " 'independent-mindedness',\n",
       " 'indicator',\n",
       " 'indifferent',\n",
       " 'induce',\n",
       " 'industriously',\n",
       " 'inevitable',\n",
       " 'inexperience',\n",
       " 'inexperienced',\n",
       " 'inflexible',\n",
       " 'influential',\n",
       " 'informality',\n",
       " 'information',\n",
       " 'ingredient',\n",
       " 'inherently',\n",
       " 'initial',\n",
       " 'initially',\n",
       " 'innocent',\n",
       " 'inoculate',\n",
       " 'inside',\n",
       " 'insight',\n",
       " 'insights',\n",
       " 'inspiration',\n",
       " 'inspiring',\n",
       " 'instance',\n",
       " 'instances',\n",
       " 'instead',\n",
       " 'intellectual',\n",
       " 'intellectually',\n",
       " 'intended',\n",
       " 'interest',\n",
       " 'interested',\n",
       " 'interesting',\n",
       " 'interestingness',\n",
       " ...]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique = set(preprocessed)\n",
    "sorted(unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5ce8fe-3a07-4f2a-90f1-a0321ce3a231",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "# 2.2 Converting tokens into token IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5204973-f414-4c0d-87b0-cfec1f06e6ff",
   "metadata": {},
   "source": [
    "- Next, we convert the text tokens into token IDs that we can process via embedding layers later\n",
    "- For this we first need to build a vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177b041d-f739-43b8-bd81-0443ae3a7f8d",
   "metadata": {},
   "source": [
    "<img src=\"figures/04.png\" width=\"900px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eeade64-037b-4b59-9039-d3b000ef8886",
   "metadata": {},
   "source": [
    "- The vocabulary contains the unique words in the input text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7fdf0533-5ab6-42a5-83fa-a3b045de6396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1897\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "77d00d96-881f-4691-bb03-84fec2a75a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {token:integer for integer,token in enumerate(all_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "41e62bfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1399"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab['qualities']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bd1f81-3a8f-4dd9-9dd6-e75f32dacbe3",
   "metadata": {},
   "source": [
    "- Below are the first 50 entries in this vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e1c5de4a-aa4e-4aec-b532-10bb364039d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('\\n', 0)\n",
      "(' ', 1)\n",
      "('!', 2)\n",
      "('\"', 3)\n",
      "(\"'\", 4)\n",
      "('(', 5)\n",
      "(')', 6)\n",
      "(',', 7)\n",
      "('.', 8)\n",
      "('14', 9)\n",
      "('15', 10)\n",
      "('18', 11)\n",
      "('21', 12)\n",
      "('50', 13)\n",
      "('7', 14)\n",
      "('99%', 15)\n",
      "(':', 16)\n",
      "(';', 17)\n",
      "('?', 18)\n",
      "('A', 19)\n",
      "('Affectation', 20)\n",
      "('Am', 21)\n",
      "('Ambition', 22)\n",
      "('Ambitious', 23)\n",
      "('An', 24)\n",
      "('And', 25)\n",
      "('Another', 26)\n",
      "('As', 27)\n",
      "('Aspies', 28)\n",
      "('At', 29)\n",
      "('Avoid', 30)\n",
      "('Be', 31)\n",
      "('Begin', 32)\n",
      "('Being', 33)\n",
      "('Believe', 34)\n",
      "('Big', 35)\n",
      "('Boldly', 36)\n",
      "('Broken', 37)\n",
      "('But', 38)\n",
      "('By', 39)\n",
      "('Can', 40)\n",
      "('Changing', 41)\n",
      "('Colleagues', 42)\n",
      "('Competition', 43)\n",
      "('Consciously', 44)\n",
      "('Copernicus', 45)\n",
      "('Corollary', 46)\n",
      "('Curiosity', 47)\n",
      "('Darwin', 48)\n",
      "('Develop', 49)\n",
      "('Do', 50)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i >= 50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1dc314-351b-476a-9459-0ec9ddc29b19",
   "metadata": {},
   "source": [
    "- Below, we illustrate the tokenization of a short sample text using a small vocabulary:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67407a9f-0202-4e7c-9ed7-1b3154191ebc",
   "metadata": {},
   "source": [
    "<img src=\"figures/05.png\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e569647-2589-4c9d-9a5c-aef1c88a0a9a",
   "metadata": {},
   "source": [
    "- Let's now put it all together into a tokenizer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f531bf46-7c25-4ef8-bff8-0d27518676d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [\n",
    "            item.strip() for item in preprocessed if item.strip()\n",
    "        ]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "        \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        # Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee7a1e5-b54f-4ca1-87ef-3d663c4ee1e7",
   "metadata": {},
   "source": [
    "- The `encode` function turns text into token IDs\n",
    "- The `decode` function turns token IDs back into text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc21d347-ec03-4823-b3d4-9d686e495617",
   "metadata": {},
   "source": [
    "<img src=\"figures/06.png\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2950a94-6b0d-474e-8ed0-66d0c3c1a95c",
   "metadata": {},
   "source": [
    "- We can use the tokenizer to encode (that is, tokenize) texts into integers\n",
    "- These integers can then be embedded (later) as input of/for the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "647364ec-7995-4654-9b4a-7607ccf5f1e4",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'qualities:'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/Users/mark/code/aia/external/LLM-workshop-2024/02_data/02.ipynb Cell 34\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mark/code/aia/external/LLM-workshop-2024/02_data/02.ipynb#X43sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m SimpleTokenizerV1(vocab)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mark/code/aia/external/LLM-workshop-2024/02_data/02.ipynb#X43sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m text \u001b[39m=\u001b[39m \u001b[39m\"\"\"\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe first step is to decide what to work on. The work you choose needs to have three qualities: it has to be something you have a natural aptitude for, that you have a deep interest in, and that offers scope to do great work.\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mark/code/aia/external/LLM-workshop-2024/02_data/02.ipynb#X43sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mark/code/aia/external/LLM-workshop-2024/02_data/02.ipynb#X43sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mIn practice you don\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt have to worry much about the third criterion. Ambitious people are if anything already too conservative about it. So all you need to do is find something you have an aptitude for and great interest in. [1]\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mark/code/aia/external/LLM-workshop-2024/02_data/02.ipynb#X43sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mark/code/aia/external/LLM-workshop-2024/02_data/02.ipynb#X43sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mThat sounds straightforward, but it\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms often quite difficult.\u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/mark/code/aia/external/LLM-workshop-2024/02_data/02.ipynb#X43sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m ids \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39;49mencode(text)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mark/code/aia/external/LLM-workshop-2024/02_data/02.ipynb#X43sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mprint\u001b[39m(ids)\n",
      "\u001b[1;32m/Users/mark/code/aia/external/LLM-workshop-2024/02_data/02.ipynb Cell 34\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mark/code/aia/external/LLM-workshop-2024/02_data/02.ipynb#X43sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m preprocessed \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msplit(\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m([,.?_!\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m()\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39m]|--|\u001b[39m\u001b[39m\\\u001b[39m\u001b[39ms)\u001b[39m\u001b[39m'\u001b[39m, text)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mark/code/aia/external/LLM-workshop-2024/02_data/02.ipynb#X43sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m preprocessed \u001b[39m=\u001b[39m [\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mark/code/aia/external/LLM-workshop-2024/02_data/02.ipynb#X43sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     item\u001b[39m.\u001b[39mstrip() \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m preprocessed \u001b[39mif\u001b[39;00m item\u001b[39m.\u001b[39mstrip()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mark/code/aia/external/LLM-workshop-2024/02_data/02.ipynb#X43sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m ]\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/mark/code/aia/external/LLM-workshop-2024/02_data/02.ipynb#X43sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m ids \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstr_to_int[s] \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m preprocessed]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mark/code/aia/external/LLM-workshop-2024/02_data/02.ipynb#X43sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mreturn\u001b[39;00m ids\n",
      "\u001b[1;32m/Users/mark/code/aia/external/LLM-workshop-2024/02_data/02.ipynb Cell 34\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mark/code/aia/external/LLM-workshop-2024/02_data/02.ipynb#X43sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m preprocessed \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msplit(\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m([,.?_!\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m()\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39m]|--|\u001b[39m\u001b[39m\\\u001b[39m\u001b[39ms)\u001b[39m\u001b[39m'\u001b[39m, text)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mark/code/aia/external/LLM-workshop-2024/02_data/02.ipynb#X43sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m preprocessed \u001b[39m=\u001b[39m [\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mark/code/aia/external/LLM-workshop-2024/02_data/02.ipynb#X43sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     item\u001b[39m.\u001b[39mstrip() \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m preprocessed \u001b[39mif\u001b[39;00m item\u001b[39m.\u001b[39mstrip()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mark/code/aia/external/LLM-workshop-2024/02_data/02.ipynb#X43sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m ]\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/mark/code/aia/external/LLM-workshop-2024/02_data/02.ipynb#X43sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m ids \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstr_to_int[s] \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m preprocessed]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mark/code/aia/external/LLM-workshop-2024/02_data/02.ipynb#X43sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mreturn\u001b[39;00m ids\n",
      "\u001b[0;31mKeyError\u001b[0m: 'qualities:'"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "\n",
    "text = \"\"\"\"The first step is to decide what to work on. The work you choose needs to have three qualities: it has to be something you have a natural aptitude for, that you have a deep interest in, and that offers scope to do great work.\n",
    "\n",
    "In practice you don't have to worry much about the third criterion. Ambitious people are if anything already too conservative about it. So all you need to do is find something you have an aptitude for and great interest in. [1]\n",
    "\n",
    "That sounds straightforward, but it's often quite difficult.\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3201706e-a487-4b60-b99d-5765865f29a0",
   "metadata": {},
   "source": [
    "- We can decode the integers back into text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01d8c8fb-432d-4a49-b332-99f23b233746",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\" It\\' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54f6aa8b-9827-412e-9035-e827296ab0fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\" It\\' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4ba34b-170f-4e71-939b-77aabb776f14",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "# 2.3 BytePair encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2309494c-79cf-4a2d-bc28-a94d602f050e",
   "metadata": {},
   "source": [
    "- GPT-2 used BytePair encoding (BPE) as its tokenizer\n",
    "- it allows the model to break down words that aren't in its predefined vocabulary into smaller subword units or even individual characters, enabling it to handle out-of-vocabulary words\n",
    "- For instance, if GPT-2's vocabulary doesn't have the word \"unfamiliarword,\" it might tokenize it as [\"unfam\", \"iliar\", \"word\"] or some other subword breakdown, depending on its trained BPE merges\n",
    "- The original BPE tokenizer can be found here: [https://github.com/openai/gpt-2/blob/master/src/encoder.py](https://github.com/openai/gpt-2/blob/master/src/encoder.py)\n",
    "- In this lecture, we are using the BPE tokenizer from OpenAI's open-source [tiktoken](https://github.com/openai/tiktoken) library, which implements its core algorithms in Rust to improve computational performance\n",
    "- (Based on an analysis [here](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch02/02_bonus_bytepair-encoder/compare-bpe-tiktoken.ipynb), I found that `tiktoken` is approx. 3x faster than the original tokenizer and 6x faster than an equivalent tokenizer in Hugging Face)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede1d41f-934b-4bf4-8184-54394a257a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "48967a77-7d17-42bf-9e92-fc619d63a59e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.8.0\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import tiktoken\n",
    "\n",
    "print(\"tiktoken version:\", importlib.metadata.version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6ad3312f-a5f7-4efc-9d7d-8ea09d7b5128",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5ff2cd85-7cfb-4325-b390-219938589428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n"
     ]
    }
   ],
   "source": [
    "text = (\n",
    "    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
    "     \"of someunknownPlace.\"\n",
    ")\n",
    "\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d26a48bb-f82e-41a8-a955-a1c9cf9d50ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunknownPlace.\n"
     ]
    }
   ],
   "source": [
    "strings = tokenizer.decode(integers)\n",
    "\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c2e7b4-6a22-42aa-8e4d-901f06378d4a",
   "metadata": {},
   "source": [
    "- BPE tokenizers break down unknown words into subwords and individual characters:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c082d41f-33d7-4827-97d8-993d5a84bb3c",
   "metadata": {},
   "source": [
    "<img src=\"figures/07.png\" width=\"700px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0beb27ee-1156-457c-839e-eebb48d94d0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[33901, 86, 343, 86, 220, 959]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"Akwirw ier\", allowed_special={\"<|endoftext|>\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbd7c0d-70f8-4386-a114-907e96c950b0",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "# 2.4 Data sampling with a sliding window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509d9826-6384-462e-aa8a-a7c73cd6aad0",
   "metadata": {},
   "source": [
    "- Above, we took care of the tokenization (converting text into word tokens represented as token ID numbers)\n",
    "- Now, let's talk about how we create the data loading for LLMs\n",
    "- We train LLMs to generate one word at a time, so we want to prepare the training data accordingly where the next word in a sequence represents the target to predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fb44f4-0c43-4a6a-9c2f-9cf31452354c",
   "metadata": {},
   "source": [
    "<img src=\"figures/08.png\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9a3d50-885b-49bc-b791-9f5cc8bc7b7c",
   "metadata": {},
   "source": [
    "- For this, we use a sliding window approach, changing the position by +1:\n",
    "\n",
    "<img src=\"figures/09.png\" width=\"900px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b006212f-de45-468d-bdee-5806216d1679",
   "metadata": {},
   "source": [
    "- Note that in practice it's best to set the stride equal to the context length so that we don't have overlaps between the inputs (the targets are still shifted by +1 always)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb467e0-bdcd-4dda-b9b0-a738c5d33ac3",
   "metadata": {},
   "source": [
    "<img src=\"figures/10.png\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fb55f51a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " tensor([[ 1532,   345,  7723,  8341],\n",
      "        [  286,  7605,   329,  1804],\n",
      "        [ 1049,   670,   287,   257],\n",
      "        [ 1256,   286,  1180,  7032],\n",
      "        [   11,   644,   561,   262],\n",
      "        [16246,   804,   588,    30],\n",
      "        [  314,  3066,   284,  1064],\n",
      "        [  503,   416,  1642,   340]])\n",
      "\n",
      "Targets:\n",
      " tensor([[  345,  7723,  8341,   286],\n",
      "        [ 7605,   329,  1804,  1049],\n",
      "        [  670,   287,   257,  1256],\n",
      "        [  286,  1180,  7032,    11],\n",
      "        [  644,   561,   262, 16246],\n",
      "        [  804,   588,    30,   314],\n",
      "        [ 3066,   284,  1064,   503],\n",
      "        [  416,  1642,   340,    13]])\n"
     ]
    }
   ],
   "source": [
    "from supplementary import create_dataloader_v1\n",
    "\n",
    "\n",
    "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=4, stride=4, shuffle=False)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "print(\"\\nTargets:\\n\", targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc671fb-6945-4594-b33f-8b462a69720d",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "# Exercise: Prepare your own favorite text dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
